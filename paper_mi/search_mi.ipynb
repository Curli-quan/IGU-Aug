{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seach $\\mu$ (minimal mutual information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    search_optimal_aug_params\n",
    "\"\"\"\n",
    "import os\n",
    "os.chdir(\"/home1/quanquan/code/landmark/code/tproj/\")\n",
    "import cv2\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import entropy\n",
    "# from scipy.misc import imread\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.exposure import histogram\n",
    "from scipy.stats import entropy\n",
    "import matplotlib\n",
    "from tutils import tfilename\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "from scipy.stats import norm\n",
    "matplotlib.use('Agg')  # Must be before importing matplotlib.pyplot or pylab!\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "EX_CONFIG = {\n",
    "    'dataset': {\n",
    "        'name': 'Cephalometric',\n",
    "        'pth': '/home1/quanquan/datasets/Cephalometric/',\n",
    "        'entr': '/home1/quanquan/datasets/Cephalometric/entr1/train/',\n",
    "        'prob': '/home1/quanquan/datasets/Cephalometric/prob/train/',\n",
    "        'n_cls': 19,\n",
    "    }\n",
    "}\n",
    "\n",
    "def test_all(landmark_list, landmark_id=0):\n",
    "    from torchvision import transforms\n",
    "    from PIL import Image\n",
    "    from tutils import tfilename\n",
    "\n",
    "    pth = EX_CONFIG['dataset']['pth']\n",
    "    if landmark_list is None:\n",
    "        from datasets.ceph.ceph_ssl import Test_Cephalometric\n",
    "        testset = Test_Cephalometric(pth, mode=\"Train\")\n",
    "        # item, landmark_list, template_patches = testset.__getitem__(0)\n",
    "        # data = testset.__getitem__(id_oneshot)\n",
    "        landmark_list = []\n",
    "        for i in range(len(testset)):\n",
    "            landmark = testset.ref_landmarks(i)[landmark_id]\n",
    "            landmark_list.append(landmark)\n",
    "\n",
    "    def get_fea(patch):\n",
    "        fea = np.zeros((256,))\n",
    "        hist, idx = histogram(patch, nbins=256)\n",
    "        for hi, idi in zip(hist, idx):\n",
    "            # print(hi, idi, i, j)\n",
    "            fea[idi] = hi\n",
    "        return fea\n",
    "\n",
    "    im_list = []\n",
    "    fea_list = []\n",
    "    # landmark_id = 3\n",
    "    for i in range(150):\n",
    "        # im_pth = tfilename(pth, \"RawImage/TrainingData\", f\"{i+1:03d}.bmp\")\n",
    "        im_pth = tfilename(pth, \"RawImage/NoisyTrainingData75\", f\"{i+1:03d}.bmp\")\n",
    "        # print(im_pth)\n",
    "        im = cv2.imread(im_pth, cv2.IMREAD_GRAYSCALE)\n",
    "        im = cv2.resize(im, (384, 384))\n",
    "        lm = landmark_list[i]\n",
    "        ps_half = 32\n",
    "        patch = im[max(lm[0]-ps_half, 0):lm[0]+ps_half, max(lm[1]-ps_half, 0):lm[1]+ps_half]\n",
    "        if not (patch.shape[0] > 0 and patch.shape[1] > 0):\n",
    "            import ipdb; ipdb.set_trace()\n",
    "        # assert patch.shape == (64, 64), f\"Got {patch.shape}\"\n",
    "        fea1 = get_fea(patch)\n",
    "        fea_list.append(fea1)\n",
    "\n",
    "    fea0 = fea_list[114]\n",
    "    # mi0 = mutual_info_score(fea0, fea0)\n",
    "    mi_list = []\n",
    "    for i in range(150):\n",
    "        mii = mutual_info_score(fea0, fea_list[i])\n",
    "        mi_list.append(mii)\n",
    "    # print(mi_list)\n",
    "    mi_list = np.array(mi_list)\n",
    "    print(\"max mi: \", mi_list.max())\n",
    "    print(\"min mi: \",  mi_list.min())\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    # draw_bar_auto_split(mi_list)\n",
    "    # return mi_list.max(), mi_list.min(), mi_list.mean()\n",
    "    return mi_list\n",
    "\n",
    "def ana_test_all():\n",
    "    res_list = []\n",
    "    for i in range(19):\n",
    "        res = test_all(None, i)\n",
    "        # print(\"---------------------------\")\n",
    "        # print(res)\n",
    "        # print(\"---------------------------\")\n",
    "        res_list.append(res)\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "    res_total = np.concatenate(res_list, axis=0)\n",
    "    print(\"res total \", res_total.mean(), res_total.std()) # ceph: mean:1.557; std:0.4429 # ceph: mean: 1.4925002126243447 std: 0.3928087723258639\n",
    "\n",
    "    # draw_bar_auto_split(res_total, tag=99)\n",
    "    # np.save(tfilename(\"./cache/res_total_lm.npy\"), res_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  2.9768203987342887\n",
      "min mi:  0.8455594107236846\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  3.478368118468378\n",
      "min mi:  0.8909906546325643\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  3.455271261551705\n",
      "min mi:  1.3368848730722867\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  2.3198318705211416\n",
      "min mi:  0.24701133950822438\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  3.2056270723603584\n",
      "min mi:  0.9116811242192856\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  2.7441647184551643\n",
      "min mi:  0.7074892629929762\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  0.9914967128900943\n",
      "min mi:  0.3303490212069548\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  1.181351692981619\n",
      "min mi:  0.42853455721494355\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  1.111468723279732\n",
      "min mi:  0.43362513798697533\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  3.0204047462765633\n",
      "min mi:  1.162504678857761\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  3.3264228893511425\n",
      "min mi:  1.0953824285111522\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  3.3416451058793273\n",
      "min mi:  0.9754327473945229\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  3.328625965731784\n",
      "min mi:  1.329921401868741\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  2.8747691215887916\n",
      "min mi:  0.5724656351814641\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  3.2966462996682186\n",
      "min mi:  1.2791012092686591\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  1.0704204115760547\n",
      "min mi:  0.28824827153416027\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  3.146106573568071\n",
      "min mi:  1.4314460507282498\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  3.3004731542438748\n",
      "min mi:  0.9252828045250657\n",
      "The sizes are set as  [384, 384]\n",
      "Initializing Datasets: (split:'Train') (len:(150)) \n",
      "max mi:  3.071227795490312\n",
      "min mi:  0.7730468702975638\n",
      "res total  1.4925002126243447 0.3928087723258639\n"
     ]
    }
   ],
   "source": [
    "ana_test_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "566aed230cdd1367dfe2c8bd7c2599f618f4b35809e6a08f2a020cd341aabc2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
